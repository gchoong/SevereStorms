id: export-noaa-storms-to-gcs
namespace: severe.storms


description: |
  Export NOAA storm data from BigQuery public dataset, run this flow in backfill mode to grab all the dates you will need. It then
  loads it to a temp table, merge into a partitioned fact table, run dbt transformations, and drop the temp table

variables:
  project: "bigquery-public-data"
  dataset: "noaa_historic_severe_storms"
  table: "storms_{{trigger.date | date('yyyy')}}"
  full_source_table: "{{ vars.project }}.{{ vars.dataset }}.{{ vars.table }}"
  destination_uri: "gs://{{ kv('GCP_BUCKET_NAME') }}/noaa_storms/storms_{{trigger.date | date('yyyy')}}.csv"
  destination_table: "{{ kv('GCP_PROJECT_ID') }}.{{ kv('GCP_DATASET') }}.temp_storms_{{trigger.date | date('yyyy')}}"
  fact_table: "{{ kv('GCP_PROJECT_ID') }}.{{ kv('GCP_DATASET') }}.noaa_storms_fact"

tasks:
  - id: query_storm_data
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: >
      SELECT * FROM `{{ render(vars.full_source_table) }}`
    destinationTable: "{{ render(vars.destination_table) }}"
    writeDisposition: WRITE_TRUNCATE
    createDisposition: CREATE_IF_NEEDED

  - id: export_to_gcs
    type: io.kestra.plugin.gcp.bigquery.ExtractToGcs
    sourceTable: "{{ render(vars.destination_table) }}"
    destinationUris:
      - "{{ render(vars.destination_uri) }}"
    format: CSV
    compression: GZIP

  - id: merge_into_fact_table
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      CREATE TABLE IF NOT EXISTS `{{ render(vars.fact_table) }}` (
        unique_row_id STRING,
        filename STRING,
        episode_id STRING,
        event_id STRING,
        state STRING,
        state_fips_code STRING,
        event_type STRING,
        cz_type STRING,
        cz_fips_code STRING,
        cz_name STRING,
        wfo STRING,
        event_begin_time DATETIME,
        event_timezone STRING,
        event_end_time DATETIME,
        injuries_direct INT64,
        injuries_indirect INT64,
        deaths_direct INT64,
        deaths_indirect INT64,
        damage_property INT64,
        damage_crops INT64,
        source STRING,
        magnitude FLOAT64,
        magnitude_type STRING,
        flood_cause STRING,
        tor_f_scale STRING,
        tor_length STRING,
        tor_width STRING,
        tor_other_wfo STRING,
        location_index STRING,
        event_range FLOAT64,
        event_azimuth STRING,
        reference_location STRING,
        event_latitude FLOAT64,
        event_longitude FLOAT64
      )
      PARTITION BY DATE(event_begin_time);

      MERGE `{{ render(vars.fact_table) }}` T
      USING (
        SELECT
          CAST(TO_HEX(MD5(CONCAT(
            COALESCE(event_id, ''),
            COALESCE(state, ''),
            COALESCE(CAST(event_begin_time AS STRING), ''),
            COALESCE(event_type, '')
          ))) AS STRING) AS unique_row_id,
          "storms_{{trigger.date | date('yyyy')}}.csv" AS filename,
          episode_id,
          event_id,
          state,
          state_fips_code,
          event_type,
          cz_type,
          cz_fips_code,
          cz_name,
          wfo,
          event_begin_time,
          event_timezone,
          event_end_time,
          injuries_direct,
          injuries_indirect,
          deaths_direct,
          deaths_indirect,
          damage_property,
          damage_crops,
          source,
          magnitude,
          magnitude_type,
          flood_cause,
          tor_f_scale,
          tor_length,
          tor_width,
          tor_other_wfo,
          location_index,
          event_range,
          event_azimuth,
          reference_location,
          event_latitude,
          event_longitude
        FROM `{{ render(vars.destination_table) }}`
      ) S
      ON T.unique_row_id = S.unique_row_id
      WHEN NOT MATCHED THEN
        INSERT ROW;

  - id: drop_temp_table
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      DROP TABLE IF EXISTS `{{ render(vars.destination_table) }}`

  - id: sync_dbt_project
    type: io.kestra.plugin.git.SyncNamespaceFiles
    url: https://github.com/gchoong/SevereStorms
    branch: main
    namespace: "{{flow.namespace}}"
    gitDirectory: dbt/severe_storms
    dryRun: false

  - id: dbt_build
    type: io.kestra.plugin.dbt.cli.DbtCLI
    description: Run dbt to transform storm fact table
    env:
      DBT_DATABASE: "{{ kv('GCP_PROJECT_ID') }}"
      DBT_SCHEMA: "{{ kv('GCP_DATASET') }}"
    namespaceFiles:
      enabled: true
    inputFiles:
      sa.json: "{{ secret('GCP_SERVICE_ACCOUNT') }}"
    containerImage: ghcr.io/kestra-io/dbt-bigquery:latest
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    commands:
      - dbt deps
      - dbt build
    storeManifest:
      key: manifest.json
      namespace: "{{ flow.namespace }}"
    profiles: |
      severe_storms:
        outputs:
          dev:
            type: bigquery
            method: service-account
            project: "{{ kv('GCP_PROJECT_ID') }}"
            dataset: "{{ kv('GCP_DATASET') }}"
            location: "{{ kv('GCP_LOCATION') }}"
            keyfile: sa.json
            threads: 4
        target: dev

pluginDefaults:
  - type: io.kestra.plugin.gcp
    values:
      serviceAccount: "{{ secret('GCP_SERVICE_ACCOUNT') }}"
      projectId: "{{ kv('GCP_PROJECT_ID') }}"
      location: "{{ kv('GCP_LOCATION') }}"
      bucket: "{{ kv('GCP_BUCKET_NAME') }}"

triggers:
  - id: yearly_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 2 2 1 *"


